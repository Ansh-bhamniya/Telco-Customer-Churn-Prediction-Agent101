{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c5b091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use this file to define pytest tests that verify the outputs of the task.\n",
    "\n",
    "This file will be copied to /tests/test_outputs.py and run by the /tests/test.sh file\n",
    "from the working directory.\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Paths used in the container environment\n",
    "VARS_PATH = Path(\"/logs/verifier/notebook_variables.json\")\n",
    "GROUND_TRUTH_PATH = Path(\"/tests/sabotaged_rows.csv\")\n",
    "AGENT_OUTPUT_PATH = Path(\"/workspace/sabotaged_rows.csv\")\n",
    "\n",
    "# Hidden Ground Truth Constants\n",
    "GT_SABOTEUR_ID = 13\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def notebook_vars():\n",
    "    \"\"\"\n",
    "    Loads the dictionary of variables captured from the agent's notebook.\n",
    "    Returns an empty dict if the file is missing.\n",
    "    \"\"\"\n",
    "    if not VARS_PATH.exists():\n",
    "        pytest.fail(f\"Variables file not found at {VARS_PATH}. Did the notebook execute?\")\n",
    "    \n",
    "    with open(VARS_PATH, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            return data\n",
    "        except json.JSONDecodeError:\n",
    "            pytest.fail(\"Failed to decode notebook_variables.json\")\n",
    "\n",
    "# ==============================================================================\n",
    "# TESTS\n",
    "# ==============================================================================\n",
    "\n",
    "def test_saboteur_identification(notebook_vars):\n",
    "    \"\"\"\n",
    "    Task 1: Tests that the agent identify the correct saboteur_id variable\n",
    "    \"\"\"\n",
    "    var_name = \"saboteur_id\"\n",
    "\n",
    "    # Check if variable exists\n",
    "    if var_name not in notebook_vars:\n",
    "        pytest.fail(f\"Variable '{var_name}' not found in notebook variables.\")\n",
    "    \n",
    "    # Check value (Allow for string or int representation)\n",
    "    try:\n",
    "        agent_val = int(notebook_vars[var_name])\n",
    "    except ValueError:\n",
    "        pytest.fail(f\"Variable '{var_name}' is not an integer.\")\n",
    "    \n",
    "    assert agent_val == GT_SABOTEUR_ID, (\n",
    "        f\"Incorrect saboteur identified. Agent found {agent_val}, expected {GT_SABOTEUR_ID}.\"\n",
    "    )\n",
    "\n",
    "def test_output_file_structure():\n",
    "    \"\"\"\n",
    "    Task 2 (Structure): Tests that the agent generate the CSV file with the correct format\n",
    "    \"\"\"\n",
    "    if not AGENT_OUTPUT_PATH.exists():\n",
    "        pytest.fail(f\"Output file '{AGENT_OUTPUT_PATH}' was not created.\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(AGENT_OUTPUT_PATH)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Could not read output CSV: {e}\")\n",
    "    \n",
    "    assert \"row_id\" in df.columns, \"Output CSV is missing the required 'row_id' column.\"\n",
    "    assert len(df) > 0, \"Output CSV is empty.\"\n",
    "\n",
    "def test_cleanup_performance_metrics():\n",
    "    \"\"\"\n",
    "    Task 2 (Logic): Check Precision (critical) and Recall.\n",
    "    \"\"\"\n",
    "    if not GROUND_TRUTH_PATH.exists():\n",
    "        pytest.fail(f\"Ground Truth file '{GROUND_TRUTH_PATH}' does not exists.\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    try:\n",
    "        ground_truth_df = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "        ground_truth_set = set(ground_truth_df['row_id'].unique())\n",
    "    except Exception:\n",
    "        pytest.fail(\"Could not ground truth data for metric calculation.\")\n",
    "        \n",
    "    # Load Agent Output\n",
    "    try:\n",
    "        agent_df = pd.read_csv(AGENT_OUTPUT_PATH)\n",
    "        agent_set = set(agent_df['row_id'].unique())\n",
    "    except Exception:\n",
    "        pytest.fail(\"Could not load agent output for metric calculation.\")\n",
    "        \n",
    "    # Calculate Intersection (True Positives)\n",
    "    true_positives = len(agent_set.intersection(ground_truth_set))\n",
    "    false_positives = len(agent_set - ground_truth_set)\n",
    "    false_negatives = len(ground_truth_set - agent_set)\n",
    "    \n",
    "    # Calculate Precision\n",
    "    # Precision = TP / (TP + FP)\n",
    "    if len(agent_set) == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        \n",
    "    # Calculate Recall\n",
    "    # Recall = TP / (TP + FN)\n",
    "    if len(ground_truth_set) == 0:\n",
    "        recall = 0.0 # Should not happen with valid data\n",
    "    else:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        \n",
    "    print(f\"\\n[METRICS] Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
    "    print(f\"[COUNTS] TP: {true_positives}, FP: {false_positives}, FN: {false_negatives}\")\n",
    "    \n",
    "    # --- ASSERTIONS ---\n",
    "    \n",
    "    # Constraint 1: Precision must be >= 0.95 (Strict Requirement)\n",
    "    assert precision >= 0.95, (\n",
    "        f\"Precision failed. Required >= 0.95, got {precision:.4f}. \"\n",
    "        \"You flagged too many honest rows as sabotage.\"\n",
    "    )\n",
    "    \n",
    "    # Constraint 2: Recall must be reasonable (e.g., > 0.5)\n",
    "    assert recall >= 0.50, (\n",
    "        f\"Recall failed. Required >= 0.50, got {recall:.4f}. \"\n",
    "        \"You missed too many sabotaged rows.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
