{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "3EOvx7ArJ3k3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff5ddb7-1891-4427-badc-8caabcbf25a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try loading from 'data/' folder (local structure) or root (Colab structure)\n",
        "try:\n",
        "    df_billing = pd.read_csv(\"data/customer_billing_churn.csv\")\n",
        "    df_profile = pd.read_csv(\"data/customer_profile.csv\")\n",
        "    print(\"Loaded files from 'data/' directory.\")\n",
        "except FileNotFoundError:\n",
        "    df_billing = pd.read_csv(\"customer_billing_churn.csv\")\n",
        "    df_profile = pd.read_csv(\"customer_profile.csv\")\n",
        "    print(\"Loaded files from root directory.\")\n",
        "\n",
        "print(\"Billing Data Shape:\", df_billing.shape)\n",
        "print(\"Profile Data Shape:\", df_profile.shape)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "u1jr5pQzJ38F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f56fc38-f7c2-428d-f49e-23f69e9bf69d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded files from root directory.\n",
            "Billing Data Shape: (7043, 7)\n",
            "Profile Data Shape: (7043, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert TotalCharges to numeric, coerce errors to NaN, fill with 0\n",
        "df_billing['TotalCharges'] = pd.to_numeric(df_billing['TotalCharges'], errors='coerce').fillna(0)\n",
        "\n",
        "# 2. Create features\n",
        "EPS = 1e-6\n",
        "df_billing['AvgMonthlySpend'] = df_billing['TotalCharges'] / (df_billing['tenure'] + EPS)\n",
        "df_billing['TenureChargeInteraction'] = df_billing['MonthlyCharges'] * df_billing['tenure']\n",
        "\n",
        "# 3. Tenure Stability Indicator\n",
        "df_billing['ShortTenureFlag'] = df_billing['tenure'].apply(lambda x: 1 if x < 12 else 0)\n",
        "\n",
        "print(\"Data preparation complete. New features created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-YJyGsVfNQmE",
        "outputId": "39e82145-ead3-419b-cde4-b811abc70145"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete. New features created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Combine on customerID\n",
        "df_merged = pd.merge(df_billing, df_profile, on='customerID')\n",
        "\n",
        "# 2. Encode Churn\n",
        "df_merged['Churn'] = df_merged['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "# 3. One-hot encode categorical variables\n",
        "# Identify categorical columns (excluding customerID)\n",
        "cat_cols = df_merged.select_dtypes(include=['object']).columns.tolist()\n",
        "if 'customerID' in cat_cols:\n",
        "    cat_cols.remove('customerID')\n",
        "\n",
        "# Apply get_dummies\n",
        "churn_model_input = pd.get_dummies(df_merged, columns=cat_cols, drop_first=False, dtype=int)\n",
        "\n",
        "print(\"Integration complete. Final dataset shape:\", churn_model_input.shape)"
      ],
      "metadata": {
        "id": "0Fgqd9ilJ4Ft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81417c6-4d69-49e7-91f3-c9b6391414f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Integration complete. Final dataset shape: (7043, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Drop customerID\n",
        "if 'customerID' in churn_model_input.columns:\n",
        "    churn_model_input = churn_model_input.drop(columns=['customerID'])\n",
        "\n",
        "# 2. Ensure no missing values\n",
        "churn_model_input = churn_model_input.dropna()\n",
        "\n",
        "# 3. Split Dataset\n",
        "X = churn_model_input.drop(columns=['Churn'])\n",
        "y = churn_model_input['Churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9h4tJj1Zkbr",
        "outputId": "46f0ac96-6cfa-4801-dc67-a25d2322d94b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (4930, 48), Test set: (2113, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit Random Forest\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = clf.predict(X_test)\n",
        "y_prob = clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate Metrics\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"Model Trained.\\nF1 Score: {f1:.5f}\\nAUC Score: {auc:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNgrMN2HZob7",
        "outputId": "ebcc2fd7-626b-4507-aa14-707835671dd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Trained.\n",
            "F1 Score: 0.57889\n",
            "AUC Score: 0.83881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Feature Importance Dictionary\n",
        "importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "feature_importance_dict = {\n",
        "    feat: round(imp, 5)\n",
        "    for feat, imp in zip(feature_names, importances)\n",
        "}\n",
        "\n",
        "# 2. Model Metrics\n",
        "model_metrics = {\n",
        "    \"f1_score\": round(f1, 5),\n",
        "    \"auc_score\": round(auc, 5)\n",
        "}\n",
        "\n",
        "# 3. Churn Counts\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "churn_counts_df = pd.DataFrame({'class': unique, 'count': counts})\n",
        "\n",
        "# Serialization\n",
        "churn_counts = churn_counts_df.to_dict(orient='split')\n",
        "\n",
        "print(\"\\n--- DELIVERABLES ---\")\n",
        "print(\"model_metrics =\", model_metrics)\n",
        "print(\"churn_counts =\", churn_counts)\n",
        "print(\"feature_importance_dict (first 5) =\", list(feature_importance_dict.items())[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr1t8nXeZs9y",
        "outputId": "90f1cdb2-bb46-4a30-bbdc-6fac2eaccc20"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DELIVERABLES ---\n",
            "model_metrics = {'f1_score': 0.57889, 'auc_score': np.float64(0.83881)}\n",
            "churn_counts = {'index': [0, 1], 'columns': ['class', 'count'], 'data': [[0, 1692], [1, 421]]}\n",
            "feature_importance_dict (first 5) = [('tenure', np.float64(0.09033)), ('MonthlyCharges', np.float64(0.09492)), ('TotalCharges', np.float64(0.1091)), ('AvgMonthlySpend', np.float64(0.0997)), ('TenureChargeInteraction', np.float64(0.10293))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify against expected values from the test script\n",
        "expected_f1 = 0.57889\n",
        "expected_auc = 0.83881\n",
        "\n",
        "print(\"--- VERIFICATION ---\")\n",
        "if abs(model_metrics['f1_score'] - expected_f1) < 0.001:\n",
        "    print(\"✅ F1 Score is correct!\")\n",
        "else:\n",
        "    print(f\"❌ F1 Score mismatch. Got {model_metrics['f1_score']}, Expected ~{expected_f1}\")\n",
        "\n",
        "if abs(model_metrics['auc_score'] - expected_auc) < 0.001:\n",
        "    print(\"✅ AUC Score is correct!\")\n",
        "else:\n",
        "    print(f\"❌ AUC Score mismatch. Got {model_metrics['auc_score']}, Expected ~{expected_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPbsxW70ZzQ9",
        "outputId": "9862cb1d-d358-488f-c1f7-bab5ef85aa04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- VERIFICATION ---\n",
            "✅ F1 Score is correct!\n",
            "✅ AUC Score is correct!\n"
          ]
        }
      ]
    }
  ]
}